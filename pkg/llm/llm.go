package llm

import (
	"context"
	"fmt"
	"log/slog"
	"time"

	jsoniter "github.com/json-iterator/go"
)

// json is used internally in the llm package for JSON processing, unifying on json-iterator
var json = jsoniter.ConfigCompatibleWithStandardLibrary

// DebugDirContextKey is the key used in context to pass the debug archive folder name
const DebugDirContextKey = "llm_debug_dir"

// LLMUsage encapsulates detailed token consumption metrics for an LLM request.
// It is used for monitoring costs, debugging context limits, and
// general observability of the model's performance.
type LLMUsage struct {
	PromptTokens     int    `json:"prompt_tokens"`               // Total tokens used in the input context
	CompletionTokens int    `json:"completion_tokens"`           // Total tokens generated by the model in the response
	TotalTokens      int    `json:"total_tokens"`                // Sum of prompt and completion tokens
	ThoughtsTokens   int    `json:"thoughts_tokens,omitempty"`   // Tokens used specifically for internal reasoning/thinking
	CachedTokens     int    `json:"cached_tokens,omitempty"`     // Number of prompt tokens that were reused from cache
	PromptDetail     string `json:"prompt_detail,omitempty"`     // Human-readable summary of prompt breakdown
	CompletionDetail string `json:"completion_detail,omitempty"` // Human-readable summary of completion specifics
	StopReason       string `json:"stop_reason,omitempty"`       // The physical reason why generation stopped (e.g., "stop", "length")
}

// LogUsage outputs usage statistics as a single structured log line.
// Parameters:
//   - model: The name of the LLM model used (for context in logs).
//   - usage: The pointer to the usage metrics to be logged.
func LogUsage(model string, usage *LLMUsage) {
	if usage == nil {
		return
	}

	attrs := []any{
		"model", model,
		"prompt", usage.PromptTokens,
		"completion", usage.CompletionTokens,
		"total", usage.TotalTokens,
		"thoughts", usage.ThoughtsTokens,
	}
	if usage.StopReason != "" {
		attrs = append(attrs, "stop", usage.StopReason)
	}
	if usage.CachedTokens > 0 {
		attrs = append(attrs, "cached", usage.CachedTokens)
	}
	slog.Info("Usage", attrs...)
}

// LLMClient serves as the primary abstraction for different LLM providers
// (e.g., Google Gemini, Ollama, OpenAI). It defines a unified way to
// interact with models using streaming chat and tool calls.
type LLMClient interface {
	// Provider returns the implementation name (e.g., "gemini", "ollama").
	Provider() string

	// StreamChat initiates a streaming conversation request.
	// Parameters:
	//   - ctx: Operation context for cancellation and timeouts.
	//   - messages: Slice of conversation history to provide as context.
	//   - availableTools: Optional tool definitions (schema) for agentic capabilities.
	// Returns:
	//   - A channel emitting StreamChunks or an error if initialization fails.
	StreamChat(ctx context.Context, messages []Message, availableTools any) (<-chan StreamChunk, error)

	// IsTransientError identifies if the given error is recoverable via retries
	// (e.g., rate limit, server overload).
	IsTransientError(err error) bool

	// SetDebug toggles the persistence of raw streaming data chunks to the disk
	// for forensic investigation of LLM behavior.
	SetDebug(enabled bool)
}

// FallbackClient implements the LLMClient interface by providing a secondary
// retry mechanism across a prioritized list of multiple LLM clients.
// It will try each client in sequence until one succeeds or all fail.
type FallbackClient struct {
	Clients    []LLMClient   // Slice of providers to attempt in order of priority
	MaxRetries int           // Number of retries to attempt per client
	RetryDelay time.Duration // Base delay between transient error retries
}

// SetDebug implements the LLMClient interface, delegating the setting to all sub-clients
func (f *FallbackClient) SetDebug(enabled bool) {
	for _, client := range f.Clients {
		client.SetDebug(enabled)
	}
}

func (f *FallbackClient) StreamChat(ctx context.Context, messages []Message, availableTools any) (<-chan StreamChunk, error) {
	var lastErr error
	for i, client := range f.Clients {
		if i > 0 {
			slog.Warn("Previous provider failed, trying fallback", "provider", i+1)
		}

		// Use the configured retry count, at least 1 attempt if set to 0
		maxRetries := f.MaxRetries
		if maxRetries <= 0 {
			maxRetries = 1
		}

		for retry := 1; retry <= maxRetries; retry++ {
			if retry > 1 {
				slog.Warn("Retrying provider", "provider", i+1, "attempt", retry, "max", maxRetries)
				// Wait briefly before retrying
				select {
				case <-ctx.Done():
					return nil, ctx.Err()
				case <-time.After(time.Duration(retry-1) * f.RetryDelay):
				}
			}

			ch, err := client.StreamChat(ctx, messages, availableTools)
			if err == nil {
				return ch, nil
			}

			lastErr = err

			// Check if the error is transient using the client's implementation
			if client.IsTransientError(err) && retry < maxRetries {
				continue
			}

			// Not a transient error, or max retries reached
			slog.Error("Provider failed", "provider", i+1, "error", err)
			break
		}
	}
	return nil, fmt.Errorf("all fallback providers failed. Last error: %v", lastErr)
}

func (f *FallbackClient) Provider() string {
	if len(f.Clients) > 0 {
		return f.Clients[0].Provider()
	}
	return "fallback"
}

// IsTransientError implements the LLMClient interface
// FallbackClient itself doesn't usually throw transient errors; they are handled by internal clients
// However, to satisfy the interface, we check if the error is considered transient by any sub-client
func (f *FallbackClient) IsTransientError(err error) bool {
	if err == nil {
		return false
	}
	for _, client := range f.Clients {
		if client.IsTransientError(err) {
			return true
		}
	}
	return false
}
