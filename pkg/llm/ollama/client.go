package ollama

import (
	"context"
	"fmt"
	"genesis/pkg/llm"
	"log"
	"net"
	"net/http"
	"net/url"
	"strings"
	"time"

	jsoniter "github.com/json-iterator/go"
	"github.com/ollama/ollama/api"
)

var json = jsoniter.ConfigCompatibleWithStandardLibrary

// OllamaClient Ollama API å®¢æˆ¶ç«¯
type OllamaClient struct {
	client  *api.Client
	model   string
	options map[string]any
}

// NewOllamaClient å‰µå»º Ollama å®¢æˆ¶ç«¯
func NewOllamaClient(model string, baseURL string, options map[string]any) (*OllamaClient, error) {
	var client *api.Client
	var err error

	// Custom Transport to ensure no timeouts are imposed by the client
	transport := &http.Transport{
		Proxy: http.ProxyFromEnvironment,
		DialContext: (&net.Dialer{
			Timeout:   30 * time.Second,
			KeepAlive: 30 * time.Second,
		}).DialContext,
		ForceAttemptHTTP2:     true,
		MaxIdleConns:          100,
		IdleConnTimeout:       90 * time.Second,
		TLSHandshakeTimeout:   10 * time.Second,
		ExpectContinueTimeout: 1 * time.Second,
		ResponseHeaderTimeout: 0, // Explicitly no timeout
	}

	customClient := &http.Client{
		Transport: transport,
		Timeout:   0, // Explicitly no timeout
	}

	if baseURL != "" {
		u, err := url.Parse(baseURL)
		if err != nil {
			return nil, fmt.Errorf("invalid base URL: %w", err)
		}
		client = api.NewClient(u, customClient)
	} else {
		// Even for environment-based, we prefer our custom client if possible
		// But api.ClientFromEnvironment creates its own client.
		// If we want to enforce our client, we should try to construct it manually if env vars are simple,
		// or just use the default fallback if baseURL is empty.
		// However, most users set baseURL in config.
		client, err = api.ClientFromEnvironment()
	}

	if err != nil {
		return nil, err
	}

	log.Printf("âœ… [Ollama] Initialized client for %s (BaseURL: %s)", model, baseURL)
	log.Printf("%+v\n", options)

	return &OllamaClient{
		client:  client,
		model:   model,
		options: options,
	}, nil
}

func (o *OllamaClient) Provider() string {
	return "ollama"
}

func (o *OllamaClient) StreamChat(ctx context.Context, messages []llm.Message, availableTools any) (<-chan llm.StreamChunk, error) {
	// è½‰æ›è¨Šæ¯
	apiMessages := o.convertMessages(messages)

	// log.Printf("[Ollama] ğŸŒŠ Tapping model: %s...", o.model)

	chunkCh := make(chan llm.StreamChunk, 100)
	startResultCh := make(chan error) // Unbuffered to detect if reader is present

	go func() {
		defer close(chunkCh)

		// è½‰æ›å·¥å…· (ä½¿ç”¨ JSON è½‰æ›ä»¥é¿é–‹ SDK é¡å‹ä¸ç›¸å®¹å•é¡Œ)
		var ollamaTools []api.Tool
		if availableTools != nil {
			log.Printf("[Ollama] ğŸ› ï¸ Converting tools of type: %T", availableTools)
			rawB, err := json.Marshal(availableTools)
			if err != nil {
				log.Printf("[Ollama] âŒ Failed to marshal tools: %v", err)
			} else {
				if err := json.Unmarshal(rawB, &ollamaTools); err != nil {
					log.Printf("[Ollama] âŒ Failed to unmarshal to api.Tool: %v", err)
				}
			}
		}

		log.Printf("[Ollama] ğŸ—ï¸ Tools available: %d", len(ollamaTools))

		streamVal := true
		req := &api.ChatRequest{
			Model:    o.model,
			Messages: apiMessages,
			Options:  o.options,
			Tools:    ollamaTools,
			Stream:   &streamVal,
		}

		started := false
		var thoughtsCount int

		err := o.client.Chat(ctx, req, func(resp api.ChatResponse) error {
			// ç¬¬ä¸€å€‹ callback è¡¨ç¤ºæˆåŠŸ
			if !started {
				started = true
				// å˜—è©¦é€šçŸ¥åˆå§‹åŒ–ï¼Œå¦‚æœæ²’äººè½(å·²Timeout)å‰‡ç•¥é
				select {
				case startResultCh <- nil:
				default:
				}
			}

			// è™•ç†æ€è€ƒå…§å®¹
			if resp.Message.Thinking != "" {
				thoughtsCount++
				chunkCh <- llm.NewThinkingChunk(resp.Message.Thinking)
			}

			// è™•ç†å›æ‡‰å…§å®¹
			if resp.Message.Content != "" {
				chunkCh <- llm.NewTextChunk(resp.Message.Content)
			}

			// è™•ç†å·¥å…·èª¿ç”¨
			if len(resp.Message.ToolCalls) > 0 {
				// log.Printf("[Ollama] ğŸ› ï¸ DEBUG: Raw ToolCalls: %+v", resp.Message.ToolCalls)
				// log.Printf("[Ollama] ğŸ› ï¸ Received ToolCalls: %d", len(resp.Message.ToolCalls))
				var toolCalls []llm.ToolCall
				for _, tc := range resp.Message.ToolCalls {
					argsB, _ := json.Marshal(tc.Function.Arguments)
					toolCalls = append(toolCalls, llm.ToolCall{
						ID:   tc.ID, // æ”¹ç‚ºæŠ“å– ID
						Name: tc.Function.Name,
						Function: llm.FunctionCall{
							Name:      tc.Function.Name,
							Arguments: string(argsB),
						},
					})
					log.Printf("[Ollama] ğŸ› ï¸ Tool Call: %s(%s) id: %s", tc.Function.Name, string(argsB), tc.ID)
				}
				chunkCh <- llm.StreamChunk{
					ToolCalls: toolCalls,
				}
			}

			// æœ€å¾Œ chunk
			if resp.Done {
				usage := &llm.LLMUsage{
					PromptTokens:     resp.PromptEvalCount,
					CompletionTokens: resp.EvalCount,
					TotalTokens:      resp.PromptEvalCount + resp.EvalCount,
					ThoughtsTokens:   thoughtsCount,
					StopReason:       resp.DoneReason,
				}

				chunkCh <- llm.NewFinalChunk(resp.DoneReason, usage)
				llm.LogUsage(o.model, usage)

				// æˆªæ–·è­¦å‘Š
				if resp.DoneReason == "length" {
					log.Printf("âš ï¸ [Ollama] Response truncated due to num_predict limit (%v)", o.options["num_predict"])
				}
			}

			return nil
		})

		if err != nil {
			log.Printf("âŒ Ollama stream error (%s): %v", o.model, err)
			if !started {
				// å˜—è©¦é€šçŸ¥åˆå§‹åŒ–ç­‰å¾…è€…
				select {
				case startResultCh <- err:
					// æˆåŠŸç™¼é€çµ¦ç­‰å¾…è€…
				default:
					// ç­‰å¾…è€…å·²è¶…æ™‚æ”¾æ£„ï¼Œæ”¹ç™¼é€éŒ¯èª¤è¨Šæ¯çµ¦ä½¿ç”¨è€…
					chunkCh <- llm.NewTextChunk(fmt.Sprintf("\nâŒ Error loading model %s: %v", o.model, err))
				}
			}
		} else if !started {
			select {
			case startResultCh <- nil:
			default:
			}
		}
	}()

	// ç­‰å¾…åˆå§‹åŒ–çµæœ
	select {
	case err := <-startResultCh:
		if err != nil {
			return nil, err
		}
		return chunkCh, nil
	case <-ctx.Done():
		return nil, ctx.Err()
	}
}

func (o *OllamaClient) convertMessages(messages []llm.Message) []api.Message {
	var ollamaMsgs []api.Message

	for _, m := range messages {
		var content strings.Builder
		var images []api.ImageData

		for _, block := range m.Content {
			switch block.Type {
			case "text", "thinking":
				// thinking å„²å­˜æ™‚åˆä½µåˆ° content
				content.WriteString(block.Text)

			case "image":
				if block.Source != nil && len(block.Source.Data) > 0 {
					images = append(images, block.Source.Data)
				}
			}
		}

		msg := api.Message{
			Role:    m.Role,
			Content: content.String(),
		}

		// è™•ç†å·¥å…·èª¿ç”¨ï¼ˆå¦‚æœæ˜¯ Assistant è§’è‰²ä¸”æœ‰ ToolCallsï¼‰
		if m.Role == "assistant" && len(m.ToolCalls) > 0 {
			var ollamaToolCalls []api.ToolCall
			for _, tc := range m.ToolCalls {
				// å°‡ JSON å­—ä¸²è½‰å› map
				var args map[string]any
				if err := json.Unmarshal([]byte(tc.Function.Arguments), &args); err != nil {
					log.Printf("[Ollama] âš ï¸ Failed to unmarshal tool arguments for history: %v", err)
				}

				// æ‰‹å‹•å»ºç«‹ api.ToolCall ä»¥ç¢ºä¿ Arguments è¢«æ­£ç¢ºè™•ç†
				// api.ToolCallFunctionArguments æ”¯æŒå¾ map ååºåˆ—åŒ–
				argBytes, _ := json.Marshal(args)
				var apiArgs api.ToolCallFunctionArguments
				_ = json.Unmarshal(argBytes, &apiArgs)

				ollamaToolCalls = append(ollamaToolCalls, api.ToolCall{
					ID: tc.ID,
					Function: api.ToolCallFunction{
						Name:      tc.Function.Name,
						Arguments: apiArgs,
					},
				})
			}
			msg.ToolCalls = ollamaToolCalls
		}

		// è™•ç†å·¥å…·çµæœï¼ˆå¦‚æœæ˜¯ Tool è§’è‰²ï¼‰
		if m.Role == "tool" {
			msg.Role = "tool"
			msg.ToolCallID = m.ToolCallID
		}

		if len(images) > 0 {
			msg.Images = images
		}

		ollamaMsgs = append(ollamaMsgs, msg)
	}

	return ollamaMsgs
}

// IsTransientError å¯¦ä½œ LLMClient ä»‹é¢
func (o *OllamaClient) IsTransientError(err error) bool {
	if err == nil {
		return false
	}
	errMsg := err.Error()

	// 1. é€£ç·šç›¸é—œéŒ¯èª¤ (Connection refused, reset)
	if strings.Contains(errMsg, "connection refused") || strings.Contains(errMsg, "connection reset") {
		return true
	}

	// 2. è² è¼‰éé‡
	if strings.Contains(strings.ToLower(errMsg), "overloaded") {
		return true
	}

	return false
}
